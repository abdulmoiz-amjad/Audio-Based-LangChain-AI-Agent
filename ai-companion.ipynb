{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install transformers tensorflow\n",
    "!pip install langchain transformers datasets peft tensorflow\n",
    "!pip install streamlit\n",
    "!pip install gtts\n",
    "!pip install pyngrok\n",
    "!pip install datasets transformers peft\n",
    "!pip install langchain transformers datasets peft tensorflow\n",
    "!pip install langchain-community\n",
    "!pip install bitsandbytes\n",
    "!pip install whisper\n",
    "!pip install streamlit soundfile\n",
    "!pip install audio-recorder-streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TFAutoModelForSequenceClassification\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from langchain import LLMChain, PromptTemplate, HuggingFacePipeline\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from audio_recorder_streamlit import audio_recorder\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import tempfile\n",
    "import torch\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import streamlit as st\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Custom CSS to style the app\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    body {\n",
    "        background-size: cover;\n",
    "    }\n",
    "    .main {\n",
    "        background: radial-gradient(circle at 10% 20%, rgb(69, 86, 102) 0%, rgb(34, 34, 34) 90%);\n",
    "        padding: 20px;\n",
    "        border-radius: 10px;\n",
    "        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);\n",
    "    }\n",
    "    .message {\n",
    "        padding: 10px;\n",
    "        border-radius: 10px;\n",
    "        margin-bottom: 10px;\n",
    "    }\n",
    "    .user {\n",
    "        background-color: #6897ab;\n",
    "    }\n",
    "    .assistant {\n",
    "        background-color: #848a86;\n",
    "        color: white;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# Load Whisper model\n",
    "@st.cache_resource\n",
    "def load_whisper_model():\n",
    "    return whisper.load_model(\"base\")  # Can switch to \"small\", \"medium\", or \"large\" models.\n",
    "\n",
    "# Whisper transcription function\n",
    "def transcribe_audio(audio_file):\n",
    "    try:\n",
    "        result = whisper_model.transcribe(audio_file.name)\n",
    "        return result[\"text\"]\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error transcribing audio: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class StoryCreativityChain:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.llmPipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=True,\n",
    "            top_k=30,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        self.llm = HuggingFacePipeline(pipeline=self.llmPipeline, model_kwargs={'temperature': 0.7, 'max_length': 5, 'top_k': 50})\n",
    "\n",
    "    def getPromptFromTemplate(self):\n",
    "        system_prompt = \"\"\"You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
    "\n",
    "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
    "        2. **Recipes:** Generate step-by-step instructions for recipes that are easy to follow, include all necessary ingredients, and result in delicious dishes. Respond to recipe-related queries such as:\n",
    "           - \"What is the recipe of...\"\n",
    "           - \"How do I make...\"\n",
    "           - \"How can I make...\"\n",
    "           - \"I want to cook...\"\n",
    "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
    "\n",
    "        For any other requests, respond politely and concisely with:\n",
    "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
    "\n",
    "        Additionally, do not generate or provide code in any programming language such as C++, Python, JavaScript, etc. If asked about coding or any other topics outside stories, recipes, and poetry, respond with:\n",
    "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
    "\n",
    "        Remember:\n",
    "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly asks questions other than these.\n",
    "        - Maintain a polite and helpful tone.\n",
    "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
    "        \"\"\"\n",
    "        \n",
    "        B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "        B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "        SYSTEM_PROMPT1 = B_SYS + system_prompt + E_SYS\n",
    "\n",
    "        instruction = \"\"\"\n",
    "        History: {history} \\n\n",
    "        User's Mood: {user's mood} \\n\n",
    "        User: {question}\"\"\"\n",
    "\n",
    "        prompt_template = B_INST + SYSTEM_PROMPT1 + instruction + E_INST\n",
    "        prompt = PromptTemplate(input_variables=[\"history\", \"question\", \"user's mood\"], template=prompt_template)\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def getNewChain(self):\n",
    "        prompt = self.getPromptFromTemplate()\n",
    "        memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\", max_len=5)\n",
    "\n",
    "        # Initialize LLMChain with proper parameters\n",
    "        llm_chain = LLMChain(prompt=prompt, llm=self.llm, verbose=True, memory=memory, output_parser=CustomOutputParser())\n",
    "\n",
    "        # Return a callable that processes inputs using the chain\n",
    "        def run_chain(inputs):\n",
    "            question = inputs.get(\"question\", \"\")\n",
    "            mood = inputs.get(\"user's mood\", \"\")\n",
    "            return llm_chain.run({\"history\": \"\", \"question\": question, \"user's mood\": mood})\n",
    "\n",
    "        return run_chain\n",
    "\n",
    "class CustomOutputParser(StrOutputParser):\n",
    "    def parse(self, response: str):\n",
    "        # Ensure response only contains content after the instruction end tag\n",
    "        return response.split('[/INST]')[-1].strip()\n",
    "\n",
    "# Cache the model and tokenizer to load them only once\n",
    "@st.cache_resource\n",
    "def load_model_and_tokenizer():\n",
    "    model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "\n",
    "    # Load LoRA configuration and apply it to the model\n",
    "    lora_config = LoraConfig.from_pretrained('/kaggle/input/fine-tuned-model2')\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Load pre-trained emotion classifier\n",
    "    emotion_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    emotion_model = TFAutoModelForSequenceClassification.from_pretrained(\"AaronMarker/emotionClassifier\", num_labels=9)\n",
    "    emotion_classifier = pipeline(\"text-classification\", model=emotion_model, tokenizer=emotion_tokenizer)\n",
    "\n",
    "    # Define the emotion mapping\n",
    "    emotions = {\n",
    "        'LABEL_0': 'Joy',\n",
    "        'LABEL_1': 'Desire',\n",
    "        'LABEL_2': 'Admiration',\n",
    "        'LABEL_3': 'Approval',\n",
    "        'LABEL_4': 'Curiosity',\n",
    "        'LABEL_5': 'Fear',\n",
    "        'LABEL_6': 'Sadness',\n",
    "        'LABEL_7': 'Anger',\n",
    "        'LABEL_8': 'Neutral'\n",
    "    }\n",
    "\n",
    "    return model, tokenizer, emotions, emotion_classifier\n",
    "\n",
    "whisper_model = load_whisper_model()\n",
    "model, tokenizer, emotions, emotion_classifier = load_model_and_tokenizer()\n",
    "\n",
    "# device = torch.device('cuda:1')\n",
    "# model.to(device)\n",
    "\n",
    "# Initialize the StoryCreativityChain\n",
    "story_chain = StoryCreativityChain(model, tokenizer)\n",
    "\n",
    "# Initialize session state if not already done\n",
    "if \"chain\" not in st.session_state:\n",
    "    st.session_state.chain = story_chain.getNewChain()\n",
    "    st.session_state.history = []\n",
    "\n",
    "# Streamlit app code\n",
    "st.title(\"Hey! How can I help you?\")\n",
    "\n",
    "# Sidebar with helper text\n",
    "st.sidebar.title(\"Creative Assistant\")\n",
    "st.sidebar.write(\"Hey, I am here to help you with amazing stories, recipes, and poetry.\")\n",
    "st.sidebar.markdown(\"**Let's get creative!**\")\n",
    "st.sidebar.header(\"Quick Tips:\")\n",
    "st.sidebar.markdown(\"\"\"\n",
    "- **For stories:** Use prompts like \"Tell me a story about a brave knight\" or \"Write a story about an adventure in space.\"\n",
    "- **For recipes:** Try asking \"Can you give me a recipe for chocolate cake?\" or \"How do I make a delicious pasta?\"\n",
    "- **For poetry:** Try prompts like \"Write a poem about love\" or \"Compose a poem about nature.\"\n",
    "- **Stay on topic:** Remember, I specialize in stories, recipes, and poetry. Let's keep our chat focused on these!\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(sentence):\n",
    "    prediction = emotions[emotion_classifier(sentence)[0][\"label\"]]\n",
    "    return prediction\n",
    "\n",
    "# Function to transcribe audio using Whisper\n",
    "def transcribe_audio(audio_file):\n",
    "    try:\n",
    "        transcription = whisper_model.transcribe(audio_file.name)\n",
    "        return transcription[\"text\"]\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error transcribing audio: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to convert text to audio\n",
    "def text_to_audio(text, filename=\"response.mp3\"):\n",
    "    tts = gTTS(text)\n",
    "    tts.save(filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Record and transcribe audio input\n",
    "audio_bytes = audio_recorder()\n",
    "if audio_bytes:\n",
    "    st.audio(audio_bytes, format=\"audio/wav\")\n",
    "    # Create a temporary file to save the audio bytes\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n",
    "        temp_audio.write(audio_bytes)  # Directly write audio_bytes\n",
    "        temp_audio_path = temp_audio  # Get the temporary file path\n",
    "    transcribed_text = transcribe_audio(temp_audio_path)  # Pass the file path to the transcription function\n",
    "    st.write(f\"**Transcribed:** {transcribed_text}\")\n",
    "else:\n",
    "    transcribed_text = \"\"\n",
    "\n",
    "# User input\n",
    "user_input = st.text_input(\"Enter your prompt:\", value=transcribed_text)\n",
    "\n",
    "if st.button(\"Generate Response\"):\n",
    "    if user_input:\n",
    "        # Add user message to history\n",
    "        st.session_state.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Generate response\n",
    "        response = st.session_state.chain({\"question\": user_input, \"user's mood\": predict_emotion(user_input)})\n",
    "        \n",
    "        # Convert response to audio\n",
    "        audio_file = text_to_audio(response)\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        st.session_state.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        # Display audio player\n",
    "        st.audio(audio_file)\n",
    "\n",
    "    else:\n",
    "        st.write(\"Please enter a prompt.\")\n",
    "\n",
    "# Display chat history\n",
    "st.write('<div class=\"main\">', unsafe_allow_html=True)\n",
    "for message in st.session_state.history:\n",
    "    role_class = \"user\" if message[\"role\"] == \"user\" else \"assistant\"\n",
    "    st.write(f'<div class=\"message {role_class}\">{message[\"content\"]}</div>', unsafe_allow_html=True)\n",
    "st.write('</div>', unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T14:52:44.911775Z",
     "iopub.status.busy": "2024-08-20T14:52:44.911384Z",
     "iopub.status.idle": "2024-08-20T14:52:46.074488Z",
     "shell.execute_reply": "2024-08-20T14:52:46.073575Z",
     "shell.execute_reply.started": "2024-08-20T14:52:44.911745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok authtoken abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T15:00:52.791366Z",
     "iopub.status.busy": "2024-08-20T15:00:52.790960Z",
     "iopub.status.idle": "2024-08-20T15:00:53.298120Z",
     "shell.execute_reply": "2024-08-20T15:00:53.297294Z",
     "shell.execute_reply.started": "2024-08-20T15:00:52.791327Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrok public URL: NgrokTunnel: \"https://017e-34-91-115-119.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Start a new ngrok tunnel\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"Ngrok public URL:\", public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T15:01:20.931512Z",
     "iopub.status.busy": "2024-08-20T15:01:20.930701Z",
     "iopub.status.idle": "2024-08-20T15:08:59.040942Z",
     "shell.execute_reply": "2024-08-20T15:08:59.040007Z",
     "shell.execute_reply.started": "2024-08-20T15:01:20.931479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8000\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8000\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.91.115.119:8000\u001b[0m\n",
      "\u001b[0m\n",
      "2024-08-20 15:01:46.561046: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-20 15:01:46.561099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-20 15:01:46.562791: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.34s/it]\n",
      "Some layers from the model checkpoint at AaronMarker/emotionClassifier were not used when initializing TFDistilBertForSequenceClassification: ['dropout_759']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at AaronMarker/emotionClassifier and are newly initialized: ['dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Generate step-by-step instructions for recipes that are easy to follow, include all necessary ingredients, and result in delicious dishes. Respond to recipe-related queries such as:\n",
      "           - \"What is the recipe of...\"\n",
      "           - \"How do I make...\"\n",
      "           - \"How can I make...\"\n",
      "           - \"I want to cook...\"\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "\n",
      "        Additionally, do not generate or provide code in any programming language such as C++, Python, JavaScript, etc. If asked about coding or any other topics outside stories, recipes, and poetry, respond with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly asks questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History:  \n",
      "\n",
      "        User's Mood: Neutral \n",
      "\n",
      "        User: Write a story about an adventure in space.[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Generate step-by-step instructions for recipes that are easy to follow, include all necessary ingredients, and result in delicious dishes. Respond to recipe-related queries such as:\n",
      "           - \"What is the recipe of...\"\n",
      "           - \"How do I make...\"\n",
      "           - \"How can I make...\"\n",
      "           - \"I want to cook...\"\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "\n",
      "        Additionally, do not generate or provide code in any programming language such as C++, Python, JavaScript, etc. If asked about coding or any other topics outside stories, recipes, and poetry, respond with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly asks questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History: Human: Write a story about an adventure in space.\n",
      "AI: Of course! Here's a story for you:\n",
      "\n",
      "Title: The Cosmic Quest\n",
      "\n",
      "In the farthest reaches of the galaxy, there existed a planet unlike any other. It was a world of wonder, where the skies shimmered with colors that defied explanation and the landscapes shifted like the tides of a cosmic ocean. This was the realm of the last surviving members of an ancient race, known only as the Cosmic Explorers.\n",
      "\n",
      "For generations, they had ventured forth into the unknown, seeking new worlds to discover and new secrets to uncover. But as the years passed, their numbers dwindled, and their legend faded into myth. That was, until one day, when a young adventurer named Astra stumbled upon their forgotten stronghold.\n",
      "\n",
      "Astra was a curious and fearless soul, with a heart full of wonder and a mind full of questions. She had grown up listening to tales of the Cosmic Explorers, and had always dreamed of joining their ranks. So when she found the ancient stronghold, she knew that her destiny was waiting for her within its crumbling walls.\n",
      "\n",
      "As she explored the ruins, Astra discovered a strange artifact - a glowing crystal that seemed to pulse with the very essence of the cosmos. It was said that this crystal held the key to unlocking the secrets of the universe, and Astra knew that she had to possess it. But she was not alone in her quest.\n",
      "\n",
      "A dark and malevolent force had also discovered the crystal's location, and it would stop at nothing to claim it for itself. Astra found herself on a cosmic adventure, navigating treacherous asteroid fields, dodging deadly space pirates, and unraveling the mysteries of the universe. Along the way, she encountered strange and wondrous creatures, each with their own unique stories to tell.\n",
      "\n",
      "As Astra journeyed deeper into the unknown, she discovered that the crystal was not just a simple artifact, but a gateway to the very fabric of reality itself. With the power to shape the cosmos at her fingertips, Astra realized that her adventure was only just beginning. The fate of the universe hung in the balance, and she was the only one who could save it from destruction.\n",
      "\n",
      "Will Astra be able to unlock the secrets of the cosmos and save the universe from destruction? The adventure continues, in a tale of wonder, magic, and the infinite possibilities of the cosmos. \n",
      "\n",
      "        User's Mood: Curiosity \n",
      "\n",
      "        User: Can you give me a recipe for chocolate cake?[/INST]\u001b[0m\n",
      "2024-08-20 15:05:48.894 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 85, in exec_func_with_error_handling\n",
      "    result = func()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 576, in code_to_exec\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/kaggle/working/streamlit_app.py\", line 191, in <module>\n",
      "    response = st.session_state.chain({\"question\": user_input, \"user's mood\": predict_emotion(user_input)})\n",
      "  File \"/kaggle/working/streamlit_app.py\", line 103, in run_chain\n",
      "    return llm_chain.run({\"history\": \"\", \"question\": question, \"user's mood\": mood})\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 598, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
      "    return self.invoke(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 750, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 944, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 787, in _generate_helper\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 774, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 271, in _generate\n",
      "    responses = self.pipeline(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 262, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1235, in __call__\n",
      "    outputs = list(final_iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\n",
      "    processed = self.infer(item, **self.params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1161, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 349, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 1638, in generate\n",
      "    outputs = self.base_model.generate(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1914, in generate\n",
      "    result = self._sample(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2651, in _sample\n",
      "    outputs = self(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1174, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 978, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 718, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 628, in forward\n",
      "    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/cache_utils.py\", line 363, in update\n",
      "    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 14.74 GiB of which 2.12 MiB is free. Process 10749 has 14.74 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 319.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run streamlit_app.py --server.port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5397932,
     "sourceId": 8967028,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5399724,
     "sourceId": 8969470,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5463880,
     "sourceId": 9060488,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
